{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Flash Attention - Overnight Build & Test\n",
    "\n",
    "**This notebook can run unattended overnight.**\n",
    "\n",
    "It builds, verifies, and saves everything automatically without requiring runtime restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Extract to LOCAL filesystem\n",
    "!unzip -q /content/drive/MyDrive/flash-attention-deterministic.zip -d /content/\n",
    "%cd /content/flash-attention-deterministic\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q ninja packaging\n",
    "\n",
    "print(\"\\n✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build (15-20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "!MAX_JOBS=16 pip install -e . --no-build-isolation -v 2>&1 | tee build.log\n",
    "\n",
    "build_time = (time.time() - start_time) / 60\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BUILD COMPLETE - took {build_time:.1f} minutes\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save Build Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save directory\n",
    "!mkdir -p /content/drive/MyDrive/flash_attn_FINAL\n",
    "\n",
    "# Save build log\n",
    "!cp build.log /content/drive/MyDrive/flash_attn_FINAL/\n",
    "\n",
    "# Save all compiled .so files\n",
    "!find . -name \"*.so\" -exec cp {} /content/drive/MyDrive/flash_attn_FINAL/ \\;\n",
    "\n",
    "# Save build directory\n",
    "!cp -r build /content/drive/MyDrive/flash_attn_FINAL/build_backup 2>/dev/null || echo \"No build dir\"\n",
    "\n",
    "print(\"\\n✓ Build artifacts saved to Drive\")\n",
    "\n",
    "# Verify compilation\n",
    "import subprocess\n",
    "flag_count = subprocess.run(['grep', '-c', 'DFLASH_ATTENTION_DETERMINISTIC', 'build.log'], \n",
    "                           capture_output=True, text=True)\n",
    "count = int(flag_count.stdout.strip() if flag_count.returncode == 0 else '0')\n",
    "print(f\"\\n-DFLASH_ATTENTION_DETERMINISTIC found: {count} times\")\n",
    "\n",
    "if count > 50:\n",
    "    print(\"✅ Compilation looks good!\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: Flag count is low\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verification Tests\n",
    "\n",
    "**NOTE:** These tests run in the same Python session as the build.\n",
    "The library should work correctly without restart since we're not replacing an existing installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Force reimport after build\n",
    "import sys\n",
    "if 'flash_attn' in sys.modules:\n",
    "    del sys.modules['flash_attn']\n",
    "    \n",
    "from flash_attn import flash_attn_func, set_deterministic_mode\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION TESTS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Memory Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(deterministic):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    set_deterministic_mode(enabled=deterministic, split_size=512)\n",
    "    \n",
    "    # CORRECT layout: (batch, seqlen, heads, headdim)\n",
    "    q = torch.randn(4, 2048, 32, 64, dtype=torch.float16, device='cuda')\n",
    "    k = torch.randn(4, 2048, 32, 64, dtype=torch.float16, device='cuda')\n",
    "    v = torch.randn(4, 2048, 32, 64, dtype=torch.float16, device='cuda')\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    before = torch.cuda.memory_allocated() / (1024**2)\n",
    "    out = flash_attn_func(q, k, v, causal=False)\n",
    "    torch.cuda.synchronize()\n",
    "    peak = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    \n",
    "    del q, k, v, out\n",
    "    torch.cuda.empty_cache()\n",
    "    return peak - before\n",
    "\n",
    "print(\"\\nMEMORY TEST (B=4, L=2048, H=32, D=64):\")\n",
    "mem_std = measure_memory(False)\n",
    "mem_det = measure_memory(True)\n",
    "\n",
    "print(f\"  Standard:      {mem_std:.2f} MB\")\n",
    "print(f\"  Deterministic: {mem_det:.2f} MB\")\n",
    "print(f\"  Increase:      {mem_det - mem_std:.2f} MB\")\n",
    "\n",
    "if mem_det - mem_std > 100:\n",
    "    print(\"  ✅ PASS: Significant memory increase detected\")\n",
    "    memory_pass = True\n",
    "else:\n",
    "    print(\"  ❌ FAIL: Memory increase too small\")\n",
    "    memory_pass = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Performance Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPERFORMANCE TEST (B=8, L=4096, H=32, D=64):\")\n",
    "\n",
    "# CORRECT layout: (batch, seqlen, heads, headdim)\n",
    "batch_size, seqlen, num_heads, head_dim = 8, 4096, 32, 64\n",
    "q = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "k = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "v = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "\n",
    "# Warmup\n",
    "set_deterministic_mode(enabled=False)\n",
    "for _ in range(10):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "set_deterministic_mode(enabled=True, split_size=512)\n",
    "for _ in range(10):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "\n",
    "# Standard mode\n",
    "set_deterministic_mode(enabled=False)\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "torch.cuda.synchronize()\n",
    "time_std = (time.perf_counter() - t0) / 50 * 1000\n",
    "\n",
    "# Deterministic mode\n",
    "set_deterministic_mode(enabled=True, split_size=512)\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "torch.cuda.synchronize()\n",
    "time_det = (time.perf_counter() - t0) / 50 * 1000\n",
    "\n",
    "overhead = ((time_det / time_std) - 1) * 100\n",
    "\n",
    "print(f\"  Standard:      {time_std:.3f} ms\")\n",
    "print(f\"  Deterministic: {time_det:.3f} ms\")\n",
    "print(f\"  Overhead:      {overhead:+.1f}%\")\n",
    "\n",
    "if 5 <= overhead <= 50:\n",
    "    print(\"  ✅ PASS: Reasonable overhead\")\n",
    "    perf_pass = True\n",
    "elif overhead > 50:\n",
    "    print(\"  ⚠️  WARNING: High overhead\")\n",
    "    perf_pass = True\n",
    "else:\n",
    "    print(\"  ⚠️  WARNING: Overhead suspiciously low\")\n",
    "    perf_pass = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Batch Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBATCH INVARIANCE TEST (B=8 vs B=4+4):\")\n",
    "\n",
    "set_deterministic_mode(enabled=True, split_size=512)\n",
    "\n",
    "# CORRECT layout: (batch, seqlen, heads, headdim)\n",
    "batch_size, seqlen, num_heads, head_dim = 8, 2048, 32, 64\n",
    "q_full = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "k_full = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "v_full = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "\n",
    "# Full batch\n",
    "out_full = flash_attn_func(q_full, k_full, v_full, causal=False)\n",
    "\n",
    "# Split batch\n",
    "out_split1 = flash_attn_func(q_full[:4], k_full[:4], v_full[:4], causal=False)\n",
    "out_split2 = flash_attn_func(q_full[4:], k_full[4:], v_full[4:], causal=False)\n",
    "out_split = torch.cat([out_split1, out_split2], dim=0)\n",
    "\n",
    "# Check\n",
    "batch_invariant = torch.equal(out_full, out_split)\n",
    "max_diff = (out_full - out_split).abs().max().item()\n",
    "\n",
    "print(f\"  Max difference: {max_diff:.2e}\")\n",
    "\n",
    "if batch_invariant:\n",
    "    print(\"  ✅ PASS: Bit-exact batch invariance\")\n",
    "    batch_pass = True\n",
    "else:\n",
    "    print(f\"  ❌ FAIL: Results differ by {max_diff}\")\n",
    "    batch_pass = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Summary and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n  Memory Test:      {'✅ PASS' if memory_pass else '❌ FAIL'}\")\n",
    "print(f\"  Performance Test: {'✅ PASS' if perf_pass else '❌ FAIL'}\")\n",
    "print(f\"  Batch Invariance: {'✅ PASS' if batch_pass else '❌ FAIL'}\")\n",
    "\n",
    "all_pass = memory_pass and perf_pass and batch_pass\n",
    "\n",
    "if all_pass:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ ALL TESTS PASSED - DETERMINISTIC MODE WORKING!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"❌ SOME TESTS FAILED\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Save detailed results\n",
    "results = f\"\"\"Deterministic Flash Attention - Build & Verification Results\n",
    "{'='*70}\n",
    "\n",
    "BUILD INFO:\n",
    "  Build time: {build_time:.1f} minutes\n",
    "  GPU: {torch.cuda.get_device_name(0)}\n",
    "  CUDA: {torch.version.cuda}\n",
    "  PyTorch: {torch.__version__}\n",
    "\n",
    "MEMORY TEST (B=4, L=2048, H=32, D=64):\n",
    "  Standard:      {mem_std:.2f} MB\n",
    "  Deterministic: {mem_det:.2f} MB\n",
    "  Increase:      {mem_det - mem_std:.2f} MB ({((mem_det/mem_std - 1) * 100):.1f}%)\n",
    "  Status: {'PASS' if memory_pass else 'FAIL'}\n",
    "\n",
    "PERFORMANCE TEST (B=8, L=4096, H=32, D=64):\n",
    "  Standard:      {time_std:.3f} ms\n",
    "  Deterministic: {time_det:.3f} ms\n",
    "  Overhead:      {overhead:+.1f}%\n",
    "  Status: {'PASS' if perf_pass else 'FAIL'}\n",
    "\n",
    "BATCH INVARIANCE TEST (B=8 vs B=4+4):\n",
    "  Max difference: {max_diff:.2e}\n",
    "  Bit-exact: {'YES' if batch_invariant else 'NO'}\n",
    "  Status: {'PASS' if batch_pass else 'FAIL'}\n",
    "\n",
    "OVERALL: {'✅ ALL TESTS PASSED' if all_pass else '❌ SOME TESTS FAILED'}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + results)\n",
    "\n",
    "# Save to Drive\n",
    "with open('/content/drive/MyDrive/flash_attn_FINAL/verification_results.txt', 'w') as f:\n",
    "    f.write(results)\n",
    "\n",
    "print(\"\\n✓ Results saved to /content/drive/MyDrive/flash_attn_FINAL/verification_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Wheel (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_pass:\n",
    "    print(\"Creating wheel package...\")\n",
    "    !python setup.py bdist_wheel\n",
    "    !cp dist/*.whl /content/drive/MyDrive/flash_attn_FINAL/\n",
    "    print(\"\\n✓ Wheel saved to Drive\")\n",
    "    print(\"\\nInstall in future sessions with:\")\n",
    "    print(\"  !pip install /content/drive/MyDrive/flash_attn_FINAL/*.whl\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping wheel creation - tests failed\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
