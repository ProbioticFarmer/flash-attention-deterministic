{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Flash Attention - Overnight Build & Test\n",
    "\n",
    "**This notebook can run unattended overnight.**\n",
    "\n",
    "It builds, verifies, and saves everything automatically without requiring runtime restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\n# Clone latest version from GitHub (includes bug fix)\n!cd /content && rm -rf flash-attention-deterministic\n!git clone https://github.com/ProbioticFarmer/flash-attention-deterministic.git /content/flash-attention-deterministic\n%cd /content/flash-attention-deterministic\n\n# Install dependencies\n!pip install -q ninja packaging\n\nprint(\"\\n✓ Setup complete\")\nprint(f\"✓ Using latest code with long sequence bug fix\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build (15-20 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "!MAX_JOBS=16 pip install -e . --no-build-isolation -v 2>&1 | tee build.log\n",
    "\n",
    "build_time = (time.time() - start_time) / 60\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BUILD COMPLETE - took {build_time:.1f} minutes\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save Build Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save directory\n",
    "!mkdir -p /content/drive/MyDrive/flash_attn_FINAL\n",
    "\n",
    "# Save build log\n",
    "!cp build.log /content/drive/MyDrive/flash_attn_FINAL/\n",
    "\n",
    "# Save all compiled .so files\n",
    "!find . -name \"*.so\" -exec cp {} /content/drive/MyDrive/flash_attn_FINAL/ \\;\n",
    "\n",
    "# Save build directory\n",
    "!cp -r build /content/drive/MyDrive/flash_attn_FINAL/build_backup 2>/dev/null || echo \"No build dir\"\n",
    "\n",
    "print(\"\\n✓ Build artifacts saved to Drive\")\n",
    "\n",
    "# Verify compilation\n",
    "import subprocess\n",
    "flag_count = subprocess.run(['grep', '-c', 'DFLASH_ATTENTION_DETERMINISTIC', 'build.log'], \n",
    "                           capture_output=True, text=True)\n",
    "count = int(flag_count.stdout.strip() if flag_count.returncode == 0 else '0')\n",
    "print(f\"\\n-DFLASH_ATTENTION_DETERMINISTIC found: {count} times\")\n",
    "\n",
    "if count > 50:\n",
    "    print(\"✅ Compilation looks good!\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: Flag count is low\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verification Tests\n",
    "\n",
    "**NOTE:** These tests run in the same Python session as the build.\n",
    "The library should work correctly without restart since we're not replacing an existing installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Force reimport after build\n",
    "import sys\n",
    "if 'flash_attn' in sys.modules:\n",
    "    del sys.modules['flash_attn']\n",
    "    \n",
    "from flash_attn import flash_attn_func, set_deterministic_mode\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION TESTS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Memory Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(deterministic):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    set_deterministic_mode(enabled=deterministic, split_size=512)\n",
    "    \n",
    "    # CORRECT layout: (batch, seqlen, heads, headdim)\n",
    "    q = torch.randn(4, 2048, 32, 64, dtype=torch.float16, device='cuda')\n",
    "    k = torch.randn(4, 2048, 32, 64, dtype=torch.float16, device='cuda')\n",
    "    v = torch.randn(4, 2048, 32, 64, dtype=torch.float16, device='cuda')\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    before = torch.cuda.memory_allocated() / (1024**2)\n",
    "    out = flash_attn_func(q, k, v, causal=False)\n",
    "    torch.cuda.synchronize()\n",
    "    peak = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    \n",
    "    del q, k, v, out\n",
    "    torch.cuda.empty_cache()\n",
    "    return peak - before\n",
    "\n",
    "print(\"\\nMEMORY TEST (B=4, L=2048, H=32, D=64):\")\n",
    "mem_std = measure_memory(False)\n",
    "mem_det = measure_memory(True)\n",
    "\n",
    "print(f\"  Standard:      {mem_std:.2f} MB\")\n",
    "print(f\"  Deterministic: {mem_det:.2f} MB\")\n",
    "print(f\"  Increase:      {mem_det - mem_std:.2f} MB\")\n",
    "\n",
    "if mem_det - mem_std > 100:\n",
    "    print(\"  ✅ PASS: Significant memory increase detected\")\n",
    "    memory_pass = True\n",
    "else:\n",
    "    print(\"  ❌ FAIL: Memory increase too small\")\n",
    "    memory_pass = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Performance Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPERFORMANCE TEST (B=8, L=4096, H=32, D=64):\")\n",
    "\n",
    "# CORRECT layout: (batch, seqlen, heads, headdim)\n",
    "batch_size, seqlen, num_heads, head_dim = 8, 4096, 32, 64\n",
    "q = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "k = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "v = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "\n",
    "# Warmup\n",
    "set_deterministic_mode(enabled=False)\n",
    "for _ in range(10):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "set_deterministic_mode(enabled=True, split_size=512)\n",
    "for _ in range(10):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "\n",
    "# Standard mode\n",
    "set_deterministic_mode(enabled=False)\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "torch.cuda.synchronize()\n",
    "time_std = (time.perf_counter() - t0) / 50 * 1000\n",
    "\n",
    "# Deterministic mode\n",
    "set_deterministic_mode(enabled=True, split_size=512)\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(50):\n",
    "    _ = flash_attn_func(q, k, v, causal=False)\n",
    "torch.cuda.synchronize()\n",
    "time_det = (time.perf_counter() - t0) / 50 * 1000\n",
    "\n",
    "overhead = ((time_det / time_std) - 1) * 100\n",
    "\n",
    "print(f\"  Standard:      {time_std:.3f} ms\")\n",
    "print(f\"  Deterministic: {time_det:.3f} ms\")\n",
    "print(f\"  Overhead:      {overhead:+.1f}%\")\n",
    "\n",
    "if 5 <= overhead <= 50:\n",
    "    print(\"  ✅ PASS: Reasonable overhead\")\n",
    "    perf_pass = True\n",
    "elif overhead > 50:\n",
    "    print(\"  ⚠️  WARNING: High overhead\")\n",
    "    perf_pass = True\n",
    "else:\n",
    "    print(\"  ⚠️  WARNING: Overhead suspiciously low\")\n",
    "    perf_pass = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Batch Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBATCH INVARIANCE TEST (B=8 vs B=4+4):\")\n",
    "\n",
    "set_deterministic_mode(enabled=True, split_size=512)\n",
    "\n",
    "# CORRECT layout: (batch, seqlen, heads, headdim)\n",
    "batch_size, seqlen, num_heads, head_dim = 8, 2048, 32, 64\n",
    "q_full = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "k_full = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "v_full = torch.randn(batch_size, seqlen, num_heads, head_dim, dtype=torch.float16, device='cuda')\n",
    "\n",
    "# Full batch\n",
    "out_full = flash_attn_func(q_full, k_full, v_full, causal=False)\n",
    "\n",
    "# Split batch\n",
    "out_split1 = flash_attn_func(q_full[:4], k_full[:4], v_full[:4], causal=False)\n",
    "out_split2 = flash_attn_func(q_full[4:], k_full[4:], v_full[4:], causal=False)\n",
    "out_split = torch.cat([out_split1, out_split2], dim=0)\n",
    "\n",
    "# Check\n",
    "batch_invariant = torch.equal(out_full, out_split)\n",
    "max_diff = (out_full - out_split).abs().max().item()\n",
    "\n",
    "print(f\"  Max difference: {max_diff:.2e}\")\n",
    "\n",
    "if batch_invariant:\n",
    "    print(\"  ✅ PASS: Bit-exact batch invariance\")\n",
    "    batch_pass = True\n",
    "else:\n",
    "    print(f\"  ❌ FAIL: Results differ by {max_diff}\")\n",
    "    batch_pass = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Memory Scaling Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nMEMORY SCALING ANALYSIS:\")\nprint(\"Testing memory overhead at increasing scales (10x, 20x, 30x...)\")\nprint(\"Test will continue until CUDA OOM or error occurs\\n\")\n\n# Baseline parameters from Test 1\nbaseline_b, baseline_l = 4, 2048\nbaseline_mem_std = mem_std\nbaseline_mem_det = mem_det\n\n# Store results\nscaling_results = []\n\n# Test at multiples of baseline\nfor scale in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n    # Scale batch size while keeping sequence length manageable\n    test_b = baseline_b * scale\n    test_l = baseline_l\n    \n    print(f\"Scale {scale}x (B={test_b}, L={test_l}):\")\n    \n    try:\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        \n        set_deterministic_mode(enabled=False)\n        q = torch.randn(test_b, test_l, 32, 64, dtype=torch.float16, device='cuda')\n        k = torch.randn(test_b, test_l, 32, 64, dtype=torch.float16, device='cuda')\n        v = torch.randn(test_b, test_l, 32, 64, dtype=torch.float16, device='cuda')\n        \n        torch.cuda.synchronize()\n        out = flash_attn_func(q, k, v, causal=False)\n        torch.cuda.synchronize()\n        mem_std_scaled = torch.cuda.max_memory_allocated() / (1024**2)\n        \n        del q, k, v, out\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        \n        set_deterministic_mode(enabled=True, split_size=512)\n        q = torch.randn(test_b, test_l, 32, 64, dtype=torch.float16, device='cuda')\n        k = torch.randn(test_b, test_l, 32, 64, dtype=torch.float16, device='cuda')\n        v = torch.randn(test_b, test_l, 32, 64, dtype=torch.float16, device='cuda')\n        \n        torch.cuda.synchronize()\n        out = flash_attn_func(q, k, v, causal=False)\n        torch.cuda.synchronize()\n        mem_det_scaled = torch.cuda.max_memory_allocated() / (1024**2)\n        \n        overhead = mem_det_scaled - mem_std_scaled\n        overhead_pct = ((mem_det_scaled / mem_std_scaled) - 1) * 100\n        \n        scaling_results.append({\n            'scale': scale,\n            'batch': test_b,\n            'seqlen': test_l,\n            'mem_std': mem_std_scaled,\n            'mem_det': mem_det_scaled,\n            'overhead_mb': overhead,\n            'overhead_pct': overhead_pct,\n            'success': True\n        })\n        \n        print(f\"  ✅ Standard: {mem_std_scaled:.1f} MB, Deterministic: {mem_det_scaled:.1f} MB\")\n        print(f\"     Overhead: +{overhead:.1f} MB ({overhead_pct:.1f}%)\\n\")\n        \n        del q, k, v, out\n        \n    except RuntimeError as e:\n        error_msg = str(e)\n        scaling_results.append({\n            'scale': scale,\n            'batch': test_b,\n            'seqlen': test_l,\n            'mem_std': None,\n            'mem_det': None,\n            'overhead_mb': None,\n            'overhead_pct': None,\n            'success': False,\n            'error': error_msg\n        })\n        \n        print(f\"  ❌ FAILED: {error_msg[:80]}...\")\n        print(f\"     Max scale reached: {scale-10}x\\n\")\n        break\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Print summary table\nprint(\"\\n\" + \"=\"*80)\nprint(\"MEMORY SCALING SUMMARY\")\nprint(\"=\"*80)\nprint(f\"{'Scale':<8} {'Batch':<8} {'Std (MB)':<12} {'Det (MB)':<12} {'Overhead':<15} {'Status':<10}\")\nprint(\"-\"*80)\n\nfor r in scaling_results:\n    if r['success']:\n        print(f\"{r['scale']:>4}x    {r['batch']:<8} {r['mem_std']:>10.1f}  {r['mem_det']:>10.1f}  +{r['overhead_mb']:>7.1f} ({r['overhead_pct']:>5.1f}%)  {'✅ OK':<10}\")\n    else:\n        print(f\"{r['scale']:>4}x    {r['batch']:<8} {'N/A':<10}  {'N/A':<10}  {'N/A':<15}  {'❌ FAIL':<10}\")\n\nprint(\"=\"*80)\n\n# Generate plot if matplotlib available\ntry:\n    import matplotlib.pyplot as plt\n    \n    successful = [r for r in scaling_results if r['success']]\n    \n    if len(successful) > 1:\n        scales = [r['scale'] for r in successful]\n        overheads_mb = [r['overhead_mb'] for r in successful]\n        overheads_pct = [r['overhead_pct'] for r in successful]\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n        \n        # Absolute overhead\n        ax1.plot(scales, overheads_mb, 'o-', linewidth=2, markersize=8)\n        ax1.set_xlabel('Scale Factor (×baseline)', fontsize=12)\n        ax1.set_ylabel('Memory Overhead (MB)', fontsize=12)\n        ax1.set_title('Deterministic Mode: Absolute Memory Overhead', fontsize=14)\n        ax1.grid(True, alpha=0.3)\n        \n        # Percentage overhead\n        ax2.plot(scales, overheads_pct, 's-', linewidth=2, markersize=8, color='orange')\n        ax2.set_xlabel('Scale Factor (×baseline)', fontsize=12)\n        ax2.set_ylabel('Memory Overhead (%)', fontsize=12)\n        ax2.set_title('Deterministic Mode: Relative Memory Overhead', fontsize=14)\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('/content/drive/MyDrive/flash_attn_FINAL/memory_scaling.png', dpi=150, bbox_inches='tight')\n        print(\"\\n✓ Memory scaling plot saved to Drive\")\n        plt.show()\n        \nexcept ImportError:\n    print(\"\\n(Matplotlib not available - skipping plot generation)\")\n\n# Save scaling data\nimport json\nwith open('/content/drive/MyDrive/flash_attn_FINAL/memory_scaling_data.json', 'w') as f:\n    json.dump(scaling_results, f, indent=2)\nprint(\"✓ Memory scaling data saved to Drive\")\n\n# Determine if scaling test passed\nmax_successful_scale = max([r['scale'] for r in scaling_results if r['success']], default=0)\nscaling_pass = max_successful_scale >= 20  # Pass if we can handle at least 20x scale\n\nprint(f\"\\nMaximum successful scale: {max_successful_scale}x\")\nif scaling_pass:\n    print(\"✅ PASS: Memory scaling test successful\")\nelse:\n    print(\"⚠️  WARNING: Low maximum scale\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Long Sequence Bug Fix Verification",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Final Results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Create Wheel (Optional)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Wheel (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_pass:\n",
    "    print(\"Creating wheel package...\")\n",
    "    !python setup.py bdist_wheel\n",
    "    !cp dist/*.whl /content/drive/MyDrive/flash_attn_FINAL/\n",
    "    print(\"\\n✓ Wheel saved to Drive\")\n",
    "    print(\"\\nInstall in future sessions with:\")\n",
    "    print(\"  !pip install /content/drive/MyDrive/flash_attn_FINAL/*.whl\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping wheel creation - tests failed\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}