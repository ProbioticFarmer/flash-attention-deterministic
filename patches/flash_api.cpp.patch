diff --git a/csrc/flash_attn/flash_api.cpp b/csrc/flash_attn/flash_api.cpp
index a7b5d36..4561adc 100644
--- a/csrc/flash_attn/flash_api.cpp
+++ b/csrc/flash_attn/flash_api.cpp
@@ -3,6 +3,7 @@
  ******************************************************************************/
 
 // Include these 2 headers instead of torch/extension.h since we don't need all of the torch headers.
+#include <string>
 #include <torch/python.h>
 #include <torch/nn/functional.h>
 #include <c10/cuda/CUDAGuard.h>
@@ -156,6 +157,13 @@ void set_params_fprop(Flash_fwd_params &params,
 
     params.unpadded_lse = unpadded_lse;
     params.seqlenq_ngroups_swapped = seqlenq_ngroups_swapped;
+
+    // [Patch] pass in deterministic flag
+    bool fa2_det = false;
+    if (const char* env = std::getenv("FA2_DETERMINISTIC")) {
+        fa2_det = (std::string(env) == "1");
+    }
+    params.deterministic = fa2_det;
 }
 
 void set_params_dgrad(Flash_bwd_params &params,
@@ -241,7 +249,17 @@ void set_params_dgrad(Flash_bwd_params &params,
 }
 
 void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream, bool force_split_kernel=false) {
+    bool fa2_det = false;
+    if (const char* env = std::getenv("FA2_DETERMINISTIC")) {
+        fa2_det = (std::string(env) == "1");
+    }
+
     FP16_SWITCH(!params.is_bf16, [&] {
+        if (fa2_det) {
+            std::cout << "[flash-attn] run_mha_fwd: num_splits=" << params.num_splits
+                      << "  force_split=" << force_split_kernel << "\n";
+        }
+
         HEADDIM_SWITCH(params.d, [&] {
             BOOL_SWITCH(params.is_causal, Is_causal, [&] {
                 if (params.num_splits <= 1 && !force_split_kernel) {  // If we don't set it num_splits == 0
@@ -311,8 +329,42 @@ std::tuple<at::Tensor, at::Tensor> set_params_splitkv(Flash_fwd_params &params,
     at::Tensor softmax_lse_accum;
     at::Tensor out_accum;
 
+    // [PATCH]: fix split-size 
+    int fixed_split_tokens = 0;
+    if (const char* env = std::getenv("FA2_SPLIT_SIZE")) {
+        try {
+            fixed_split_tokens = std::max(0, std::stoi(env));
+        } catch (...) {
+            fixed_split_tokens = 0;
+        }
+    }
+    bool fa2_det = false;
+    if (const char* env_det = std::getenv("FA2_DETERMINISTIC")) {
+        fa2_det = (std::string(env_det) == "1");
+    }
+    // bool used_fixed_split = false;
+
+    if (p_dropout == 0.0f && fixed_split_tokens > 0 && params.num_splits < 1 && fa2_det) {
+        int blocks_per_split = (fixed_split_tokens + block_n - 1) / block_n;
+        blocks_per_split = std::max(1, blocks_per_split);
+
+        int computed_splits = (num_n_blocks + blocks_per_split - 1) / blocks_per_split;
+        params.num_splits = std::max(1, computed_splits);
+        // used_fixed_split = true;
+    }
+
+    if (fa2_det) {
+        std::cout << "[flash-attn] set_params_splitkv: "
+                  << " B=" << batch_size
+                  << " S_k=" << max_seqlen_k
+                  << " S_q=" << max_seqlen_q
+                  << " n_blocks" << num_n_blocks
+                  << " num_splits=" << params.num_splits
+                  << " fixed_tokens=" << fixed_split_tokens << "\n";
+    }
+
     if (p_dropout == 0.0f) {  // SplitKV is not implemented for dropout
-        if (num_splits < 1) {
+        if (params.num_splits < 1) {
             // We multiply number of SMs by 2 to hard-code the fact that we're using 128 threads per block.
             params.num_splits = num_splits_heuristic(batch_size * num_heads * num_m_blocks, num_sm * 2, num_n_blocks, 128);
         }
@@ -325,6 +377,12 @@ std::tuple<at::Tensor, at::Tensor> set_params_splitkv(Flash_fwd_params &params,
         TORCH_CHECK(params.num_splits <= 128, "num_splits > 128 not supported");
     }
 
+    // printf(
+    //     "[FA2] set_params_splitkv: B=%d Hq=%d D=%d max_k=%d block_n%d n_blocks=%d fixed_tokens=%d used_fixed=%d -> num_splits=%d p_drop%.1f\n",
+    //     batch_size, num_heads, head_size, max_seqlen_k, block_n, num_n_blocks, fixed_split_tokens, used_fixed_split ? 1 : 0, params.num_splits, p_dropout
+    // );
+    // fflush(stdout);
+
     return std::make_tuple(softmax_lse_accum, out_accum);
 }
 
@@ -503,6 +561,27 @@ mha_fwd(at::Tensor &q,         // batch_size x seqlen_q x num_heads x round_mult
         softmax_lse.fill_(std::numeric_limits<float>::infinity());
     }
 
+    bool fa2_det = false;
+    if (const char* env = std::getenv("FA2_DETERMINISTIC")) {
+        fa2_det = (std::string(env) == "1");
+    }
+    if (fa2_det && params.num_splits > 1) {
+        static std::once_flag warned;
+        std::call_once(warned, []() {
+            std::cout << "[flash-attn] FA2 deterministic parallel reduction active for split-k\n";
+        });
+        // TML-style deterministic parallel reduction using PyTorch ops
+        // These operations are deterministic when executed in fixed order on same device
+        if (out_accum.defined() && softmax_lse_accum.defined()) {
+            at::Tensor lse_total = at::logsumexp(softmax_lse_accum, 0);
+            softmax_lse.copy_(lse_total);
+            at::Tensor weights = at::exp(softmax_lse_accum - lse_total.unsqueeze(0));
+            at::Tensor weighted = weights.unsqueeze(-1) * out_accum;
+            at::Tensor out_sum = weighted.sum(0);
+            out.copy_(out_sum.permute({0, 2, 1, 3}).to(out.dtype()));
+        }
+    }
+
     if (seqlenq_ngroups_swapped) {
         out = out.transpose(1, 2).reshape({batch_size, 1, num_heads_k * seqlen_q, head_size});
         q = q.transpose(1, 2).reshape({batch_size, 1, num_heads_k * seqlen_q, head_size});
@@ -743,6 +822,27 @@ mha_varlen_fwd(at::Tensor &q,  // total_q x num_heads x head_size, total_q := \s
         softmax_lse.fill_(std::numeric_limits<float>::infinity());
     }
 
+    bool fa2_det = false;
+    if (const char* env = std::getenv("FA2_DETERMINISTIC")) {
+        fa2_det = (std::string(env) == "1");
+    }
+    if (fa2_det && params.num_splits > 1) {
+        static std::once_flag warned;
+        std::call_once(warned, []() {
+            std::cout << "[flash-attn] FA2 deterministic parallel reduction active for split-k\n";
+        });
+        // TML-style deterministic parallel reduction using PyTorch ops
+        // These operations are deterministic when executed in fixed order on same device
+        if (out_accum.defined() && softmax_lse_accum.defined()) {
+            at::Tensor lse_total = at::logsumexp(softmax_lse_accum, 0);
+            softmax_lse.copy_(lse_total);
+            at::Tensor weights = at::exp(softmax_lse_accum - lse_total.unsqueeze(0));
+            at::Tensor weighted = weights.unsqueeze(-1) * out_accum;
+            at::Tensor out_sum = weighted.sum(0);
+            out.copy_(out_sum.permute({0, 2, 1, 3}).to(out.dtype()));
+        }
+    }
+
     if (seqlenq_ngroups_swapped) {
         int64_t size_before[] = {batch_size, max_seqlen_q, num_heads_k, head_size};
         int64_t size_after[] = {batch_size, num_heads_k * max_seqlen_q, head_size};
@@ -1456,6 +1556,27 @@ mha_fwd_kvcache(at::Tensor &q,                 // batch_size x seqlen_q x num_he
     // or paged KV cache
     run_mha_fwd(params, stream, /*force_split_kernel=*/k_.has_value() || cache_batch_idx_.has_value() || paged_KV);
 
+    bool fa2_det = false;
+    if (const char* env = std::getenv("FA2_DETERMINISTIC")) {
+        fa2_det = (std::string(env) == "1");
+    }
+    if (fa2_det && params.num_splits > 1) {
+        static std::once_flag warned;
+        std::call_once(warned, []() {
+            std::cout << "[flash-attn] FA2 deterministic parallel reduction active for split-k\n";
+        });
+        // TML-style deterministic parallel reduction using PyTorch ops
+        // These operations are deterministic when executed in fixed order on same device
+        if (out_accum.defined() && softmax_lse_accum.defined()) {
+            at::Tensor lse_total = at::logsumexp(softmax_lse_accum, 0);
+            softmax_lse.copy_(lse_total);
+            at::Tensor weights = at::exp(softmax_lse_accum - lse_total.unsqueeze(0));
+            at::Tensor weighted = weights.unsqueeze(-1) * out_accum;
+            at::Tensor out_sum = weighted.sum(0);
+            out.copy_(out_sum.permute({0, 2, 1, 3}).to(out.dtype()));
+        }
+    }
+
     if (head_size_og % 8 != 0) {
         out = out.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
         if (out_.has_value()) { out_.value().copy_(out); }
